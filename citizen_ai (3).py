# -*- coding: utf-8 -*-
"""Citizen AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hzJRxNoS71i0-ROyrCZeez7yZ9Qq5wkD
"""

!pip install transformers torch gradio -q

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import gradio as gr

# Load IBM Granite model from Hugging Face
model_name = "ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Create text generation pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Function for chatbot
def ask_citizen_ai(query):
    response = generator(query, max_length=200, num_return_sequences=1)
    return response[0]['generated_text']

# Gradio UI
interface = gr.Interface(
    fn=ask_citizen_ai,
    inputs="text",
    outputs="text",
    title="Citizen AI - IBM Granite"
)

interface.launch()